# AI LLM Vulnerability Scanner

A lightweight Streamlit application to evaluate Large Language Models (LLMs) for insecure or undesired behavior using configurable test packs (JSON plugins). The app runs prompt-based tests against a target LLM, evaluates responses (LLM-as-judge with a keyword fallback), and produces interactive dashboards, timestamped JSON scan results, and an executive PDF report.

**Repository layout**
- [app.py](app.py): Streamlit UI — Live Chat, Test Scenarios, Dashboard, and Scan History.
- [agent.py](agent.py): `ScanAgent` — runs tests from a scan pack and yields verdicts.
- [scanner.py](scanner.py): Test evaluation logic (LLM-as-judge → JSON extraction; keyword fallback).
- [llm_client.py](llm_client.py): Minimal HTTP client for LLM chat completions (expects OpenAI-like `/chat/completions`).
- [reporter.py](reporter.py): JSON report generator saved to `reports/`.
- [pdf_report.py](pdf_report.py): Executive PDF generator using ReportLab.
- [dashboard.py](dashboard.py): Dashboard visuals (Altair + Pandas).
- [plugins/](plugins/): Scan packs (e.g., `owasp_llm_top10.json`).
- [scan_results/](scan_results/): Saved scan JSON files (timestamped).
- [reports/](reports/): JSON and PDF reports produced by the app.
- [requirements.txt](requirements.txt): Python dependencies.
- [llm_config.json](llm_config.json): Default LLM + judge configuration used by the app.

**DEMO URL**
https://ai-pentest.streamlit.app/

## Quick Start

Follow these steps to set up and run the app. Examples include generic steps and platform-specific activation commands for Windows, macOS, and Linux.

1. Create a virtual environment (generic):

```bash
python -m venv .venv
```

2. Activate the virtual environment:

- Windows (PowerShell):

```powershell
.venv\Scripts\Activate.ps1
```

- Windows (cmd):

```cmd
.venv\Scripts\activate
```

- macOS / Linux (bash or zsh):

```bash
source .venv/bin/activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Configure the LLM endpoints:
- Edit [llm_config.json](llm_config.json) and set `base_url`, `model`, and `api_key`.
- The file also supports a `judge` object (used to evaluate responses). Example keys: `base_url`, `model`, `api_key`, `timeout`, `auth_header`, `auth_prefix`.

5. Run the Streamlit UI:

```bash
streamlit run app.py
```

Open the browser URL printed by Streamlit. Use the sidebar to update/save LLM configuration and run the LLM health check.

## Main UI features
- Live Chat: interact with the configured target LLM.
- Test Scenarios: pick a scan pack from `plugins/` (default: OWASP LLM Top 10) and run the tests.
- Dashboard: visualize severity/category distribution for the latest scan results.
- Scan History: browse previously saved JSON scans in `scan_results/` and download PDFs from `reports/`.

When a scan runs, results are saved as `scan_results/scan_<timestamp>.json`. The UI also offers an executive PDF generated by `pdf_report.py`.

## Plugin format (scan packs)
Plugins live in `plugins/` and are JSON objects with a `scan_name` and `tests` array. Each test contains keys such as:
- `id`: unique test id (e.g., `LLM01`)
- `category`: human-friendly category (e.g., "Prompt Injection")
- `prompt` or `prompts` (per-language)
- `fail_keywords`: list (or map by language) used by the keyword fallback
- `compliance`: optional mapping to frameworks

See the default: [plugins/owasp_llm_top10.json](plugins/owasp_llm_top10.json).

## Programmatic usage (run scans from Python)
You can import `LLMClient`, `ScanAgent`, and `evaluate_test` to run scans without the UI. Minimal example:

```python
from llm_client import LLMClient
from agent import ScanAgent
from scanner import evaluate_test
import json

cfg = json.load(open('llm_config.json'))
llm = LLMClient(cfg)
judge = LLMClient(cfg['judge'])
scan = json.load(open('plugins/owasp_llm_top10.json'))

agent = ScanAgent(llm, judge, scan)
results = list(agent.run(evaluate_test))
print(results)
```

Notes:
- `LLMClient.chat(prompt)` expects the LLM endpoint to return an OpenAI-like `/chat/completions` JSON with `choices[0].message.content`.
- `scanner.evaluate_test()` will first attempt to have the judge LLM return a JSON verdict; if that fails it uses `fail_keywords` for detection.

## Files & outputs
- Scan results: saved to `scan_results/scan_<timestamp>.json` (used by the Dashboard and Scan History).
- Executive PDF: created by `pdf_report.py` in `reports/`.
- JSON report generator: `reporter.generate_report()` writes `reports/report_<scan_name>.json`.

## Configuration tips & troubleshooting
- If the Streamlit sidebar health check fails: verify `base_url` and `model`, and ensure the LLM endpoint implements `/chat/completions`.
- To adapt to a different API shape, modify `llm_client.py` (headers, auth format, or response parsing).
- If the judge LLM produces non-JSON output, the judge extraction may fail—`scanner.py` falls back to keyword matching.

## Development & extending
- Add or modify plugins inside `plugins/` to change test coverage or add languages.
- Tweak `pdf_report.py` and `dashboard.py` for custom reporting or visualizations.

---

## Update plugin from Excel

There is a helper script to bulk-add tests from an Excel file into an existing plugin JSON: [update_plugin_from_excel.py](update_plugin_from_excel.py).

Purpose
- Convert rows from an Excel sheet into new tests and append them to a plugin file.

Usage

```bash
python update_plugin_from_excel.py <plugin.json> <tests.xlsx>
```

Example

```bash
python update_plugin_from_excel.py plugins/owasp_llm_top10.json test_cases.xlsx
```

Expected Excel format
- Required columns (exact names expected by the script):
	- `id`
	- `category`
	- `prompt`
	- `failed_keywords` (comma-separated)
	- `compliance(OWASP)` (comma-separated)
	- `compliance(DPDP)` (comma-separated)
	- `compliance(GDPR)` (comma-separated)
	- `compliance(ISO27001)` (comma-separated)

Behavior
- Rows with duplicate `id` or a `prompt` that normalizes to an existing prompt are skipped.
- `failed_keywords` and compliance columns are parsed by splitting on commas; empty cells become empty lists.
- When new tests are added the plugin file is overwritten (so consider making a backup first).
- The script prints a summary showing how many tests were added and how many rows were rejected.

Notes & suggestions
- The script uses `pandas.read_excel()` (requires `openpyxl` for `.xlsx`), so ensure dependencies from `requirements.txt` are installed.
- If your Excel uses different column names or CSV format, the script will need small edits; I can update it to accept aliases, CSV input, `--dry-run`, and automatic backups if you want.
